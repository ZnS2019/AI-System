# Lab 2 - 定制一个新的张量运算

## 实验目的

1.	理解DNN框架中的张量算子的原理
2.	基于不同方法实现新的张量运算，并比较性能差异

## 实验环境

* PyTorch==1.5.0

## 实验原理

1. 深度神经网络中的张量运算原理
2. PyTorch中基于Function和Module构造张量的方法
3. 通过C++扩展编写Python函数模块

## 实验内容

### 实验流程图

![](/imgs/Lab2-flow.png "Lab2 flow chat")

### 具体步骤

1.	在MNIST的模型样例中，选择线性层（Linear）张量运算进行定制化实现

2.	理解PyTorch构造张量运算的基本单位：Function和Module

3.	基于Function和Module的Python API重新实现Linear张量运算

    1. 修改MNIST样例代码
    2. 基于PyTorch  Module编写自定义的Linear 类模块
    3. 基于PyTorch Function实现前向计算和反向传播函数
    4. 使用自定义Linear替换网络中nn.Linear() 类
    5. 运行程序，验证网络正确性
   
4.	理解PyTorch张量运算在后端执行原理

5.	实现C++版本的定制化张量运算

    1. 基于C++，实现自定义Linear层前向计算和反向传播函数，并绑定为Python模型
    2. 将代码生成python的C++扩展
    3. 使用基于C++的函数扩展，实现自定义Linear类模块的前向计算和反向传播函数
    4. 运行程序，验证网络正确性
   
6.	使用profiler比较网络性能：比较原有张量运算和两种自定义张量运算的性能

7.	【可选实验，加分】实现卷积层（Convolutional）的自定义张量运算


## 实验报告

### 实验环境

||||
|--------|--------------|--------------------------|
|硬件环境|CPU（vCPU数目）|&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Intel® Core™ i7-9750H CPU @ 2.60GHz × 12&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |
||GPU(型号，数目)|GeForce GTX 1660 Ti with Max-Q Design/PCIe/SSE2|
|软件环境|OS版本|ubuntu 18.04 LTS|
||深度学习框架<br>python包名称及版本|pytorch 1.5.0|
||CUDA版本|cuda 10.1|
||||

### 实验结果

|||
|---------------|---------------------------|
| 实现方式（Linear层为例）| &nbsp; &nbsp; &nbsp; &nbsp; 性能评测 |
|<br/> <br/>PyTorch原有张量运算<br/> <br/>&nbsp;|![image-20210314112505351](lab2_2183221373_周乃松_20210314.assets/image-20210314112505351.png) |
|<br/> <br/>基于Python API的定制化张量运算<br/> <br/>&nbsp;|![image-20210314112139735](lab2_2183221373_周乃松_20210314.assets/image-20210314112139735.png)|
|<br/> <br/>基于C++的定制化张量运算<br/> <br/>&nbsp;|![image-20210314112407070](lab2_2183221373_周乃松_20210314.assets/image-20210314112407070.png)|
|||

结果分析：

1. 整体上看，pytorch 原有张量运算、基于python api定制的张量运算和基于c++的定制张量运算，它们的整体运行时间都相差不大，cpu time 在150ms到200ms之间，而cuda time 一般在640ms左右。
2. 在profiler的输出按cpu time 排序中，原有的张量运算前十名中没有linear层，而经过定制的张量运算中，myLinearFunction都可以排到前十当中。这说明pytorch本身的优化是比我实验中利用高级语言接口定制的要好的。在对pytorch源码的追溯过程中，我发现nn.Linear的底层都是调用的封装好的c++接口，更底层没有找到，很可能以直接编写cuda的方式进行了优化，因此实现效率更高也是很自然的了。
3. 但是，线性层本身比起卷积层来说并不复杂，占用的计算资源不会很多，因此三者虽然在优化效率上有区别，但总体时间差别并不大，并且具有很大的随机性。
4. 在自定制的张量运算中，还加入了偏置的操作，整体实现时间没有明显区别。